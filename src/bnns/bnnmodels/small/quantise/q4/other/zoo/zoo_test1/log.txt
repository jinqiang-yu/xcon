2022-01-29 12:54:18 - INFO - saving to ./results/small/quantise/q4/other/zoo/zoo_test1/
2022-01-29 12:54:18 - DEBUG - run arguments: Namespace(config='./configs/config_small.json', data='../../paper_bench/complete/quantise/q4/other/zoo/zoo_data.csv', encode=None, id=0, load=None, neighprec=-1, results='./results/small/quantise/q4/other/zoo/zoo_test1/', test='../../paper_bench/cv/test/quantise/q4/other/zoo/zoo_test1_data.csv', train='../../paper_bench/cv/train/quantise/q4/other/zoo/zoo_train1_data.csv')
2022-01-29 12:54:18 - INFO - creating model mlp_binary
2022-01-29 12:54:18 - INFO - created model with configuration: {'name': 'mlp_binary', 'type': 'cpu', 'type_model': 'torch.FloatTensor', 'layers': [10, 5, 5, 7]}
2022-01-29 12:54:18 - INFO - number of parameters: 517
2022-01-29 12:54:18 - INFO - training regime: {0: {'optimizer': 'Adam', 'lr': 0.025, 'betas': (0.9, 0.999)}, 20: {'lr': 0.0025}, 50: {'lr': 0.00025}, 75: {'lr': 2.5e-05}}
2022-01-29 12:54:18 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:18 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:18 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:19 - INFO - TRAINING - Epoch: [0][0/2]	Time 0.265 (0.265)	Data 0.211 (0.211)	Loss 3.1185 (3.1185)	Prec@1 14.062 (14.062)	
2022-01-29 12:54:19 - INFO - EVALUATING - Epoch: [0][0/1]	Time 0.200 (0.200)	Data 0.198 (0.198)	Loss 1.4312 (1.4312)	Prec@1 57.143 (57.143)	
2022-01-29 12:54:19 - INFO - 
 Epoch: 1	Training Loss 3.2415 	Training Prec@1 12.500 	Validation Loss 1.4312 	Validation Prec@1 57.143 	
2022-01-29 12:54:19 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:19 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:19 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:19 - INFO - TRAINING - Epoch: [1][0/2]	Time 0.192 (0.192)	Data 0.189 (0.189)	Loss 2.5648 (2.5648)	Prec@1 34.375 (34.375)	
2022-01-29 12:54:19 - INFO - EVALUATING - Epoch: [1][0/1]	Time 0.213 (0.213)	Data 0.212 (0.212)	Loss 1.2064 (1.2064)	Prec@1 71.429 (71.429)	
2022-01-29 12:54:19 - INFO - 
 Epoch: 2	Training Loss 2.4423 	Training Prec@1 37.500 	Validation Loss 1.2064 	Validation Prec@1 71.429 	
2022-01-29 12:54:19 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:19 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:19 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:19 - INFO - TRAINING - Epoch: [2][0/2]	Time 0.192 (0.192)	Data 0.189 (0.189)	Loss 1.4698 (1.4698)	Prec@1 46.875 (46.875)	
2022-01-29 12:54:20 - INFO - EVALUATING - Epoch: [2][0/1]	Time 0.193 (0.193)	Data 0.192 (0.192)	Loss 0.6564 (0.6564)	Prec@1 80.952 (80.952)	
2022-01-29 12:54:20 - INFO - 
 Epoch: 3	Training Loss 1.5337 	Training Prec@1 42.500 	Validation Loss 0.6564 	Validation Prec@1 80.952 	
2022-01-29 12:54:20 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:20 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:20 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:20 - INFO - TRAINING - Epoch: [3][0/2]	Time 0.244 (0.244)	Data 0.240 (0.240)	Loss 1.4408 (1.4408)	Prec@1 51.562 (51.562)	
2022-01-29 12:54:20 - INFO - EVALUATING - Epoch: [3][0/1]	Time 0.188 (0.188)	Data 0.186 (0.186)	Loss 1.1900 (1.1900)	Prec@1 61.905 (61.905)	
2022-01-29 12:54:20 - INFO - 
 Epoch: 4	Training Loss 1.5015 	Training Prec@1 55.000 	Validation Loss 1.1900 	Validation Prec@1 61.905 	
2022-01-29 12:54:20 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:20 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:20 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:20 - INFO - TRAINING - Epoch: [4][0/2]	Time 0.195 (0.195)	Data 0.191 (0.191)	Loss 1.6653 (1.6653)	Prec@1 54.688 (54.688)	
2022-01-29 12:54:21 - INFO - EVALUATING - Epoch: [4][0/1]	Time 0.193 (0.193)	Data 0.191 (0.191)	Loss 0.8202 (0.8202)	Prec@1 66.667 (66.667)	
2022-01-29 12:54:21 - INFO - 
 Epoch: 5	Training Loss 1.5076 	Training Prec@1 57.500 	Validation Loss 0.8202 	Validation Prec@1 66.667 	
2022-01-29 12:54:21 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:21 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:21 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:21 - INFO - TRAINING - Epoch: [5][0/2]	Time 0.203 (0.203)	Data 0.199 (0.199)	Loss 2.3249 (2.3249)	Prec@1 50.000 (50.000)	
2022-01-29 12:54:21 - INFO - EVALUATING - Epoch: [5][0/1]	Time 0.193 (0.193)	Data 0.191 (0.191)	Loss 2.1499 (2.1499)	Prec@1 9.524 (9.524)	
2022-01-29 12:54:21 - INFO - 
 Epoch: 6	Training Loss 2.3834 	Training Prec@1 51.250 	Validation Loss 2.1499 	Validation Prec@1 9.524 	
2022-01-29 12:54:21 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:21 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:21 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:21 - INFO - TRAINING - Epoch: [6][0/2]	Time 0.206 (0.206)	Data 0.203 (0.203)	Loss 1.8306 (1.8306)	Prec@1 12.500 (12.500)	
2022-01-29 12:54:21 - INFO - EVALUATING - Epoch: [6][0/1]	Time 0.197 (0.197)	Data 0.196 (0.196)	Loss 0.7113 (0.7113)	Prec@1 71.429 (71.429)	
2022-01-29 12:54:21 - INFO - 
 Epoch: 7	Training Loss 1.7489 	Training Prec@1 22.500 	Validation Loss 0.7113 	Validation Prec@1 71.429 	
2022-01-29 12:54:21 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:21 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:21 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:22 - INFO - TRAINING - Epoch: [7][0/2]	Time 0.239 (0.239)	Data 0.235 (0.235)	Loss 1.6035 (1.6035)	Prec@1 51.562 (51.562)	
2022-01-29 12:54:22 - INFO - EVALUATING - Epoch: [7][0/1]	Time 0.198 (0.198)	Data 0.196 (0.196)	Loss 0.9465 (0.9465)	Prec@1 42.857 (42.857)	
2022-01-29 12:54:22 - INFO - 
 Epoch: 8	Training Loss 1.7014 	Training Prec@1 51.250 	Validation Loss 0.9465 	Validation Prec@1 42.857 	
2022-01-29 12:54:22 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:22 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:22 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:22 - INFO - TRAINING - Epoch: [8][0/2]	Time 0.197 (0.197)	Data 0.194 (0.194)	Loss 1.4155 (1.4155)	Prec@1 37.500 (37.500)	
2022-01-29 12:54:22 - INFO - EVALUATING - Epoch: [8][0/1]	Time 0.197 (0.197)	Data 0.196 (0.196)	Loss 0.5272 (0.5272)	Prec@1 80.952 (80.952)	
2022-01-29 12:54:22 - INFO - 
 Epoch: 9	Training Loss 1.4104 	Training Prec@1 43.750 	Validation Loss 0.5272 	Validation Prec@1 80.952 	
2022-01-29 12:54:22 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:22 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:22 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:23 - INFO - TRAINING - Epoch: [9][0/2]	Time 0.192 (0.192)	Data 0.189 (0.189)	Loss 1.2976 (1.2976)	Prec@1 60.938 (60.938)	
2022-01-29 12:54:23 - INFO - EVALUATING - Epoch: [9][0/1]	Time 0.193 (0.193)	Data 0.192 (0.192)	Loss 0.8590 (0.8590)	Prec@1 71.429 (71.429)	
2022-01-29 12:54:23 - INFO - 
 Epoch: 10	Training Loss 1.2086 	Training Prec@1 61.250 	Validation Loss 0.8590 	Validation Prec@1 71.429 	
2022-01-29 12:54:23 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:23 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:23 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:23 - INFO - TRAINING - Epoch: [10][0/2]	Time 0.197 (0.197)	Data 0.193 (0.193)	Loss 1.8225 (1.8225)	Prec@1 50.000 (50.000)	
2022-01-29 12:54:23 - INFO - EVALUATING - Epoch: [10][0/1]	Time 0.198 (0.198)	Data 0.196 (0.196)	Loss 0.7754 (0.7754)	Prec@1 76.190 (76.190)	
2022-01-29 12:54:23 - INFO - 
 Epoch: 11	Training Loss 1.5781 	Training Prec@1 53.750 	Validation Loss 0.7754 	Validation Prec@1 76.190 	
2022-01-29 12:54:23 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:23 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:23 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:23 - INFO - TRAINING - Epoch: [11][0/2]	Time 0.199 (0.199)	Data 0.195 (0.195)	Loss 1.2292 (1.2292)	Prec@1 67.188 (67.188)	
2022-01-29 12:54:24 - INFO - EVALUATING - Epoch: [11][0/1]	Time 0.193 (0.193)	Data 0.191 (0.191)	Loss 0.5654 (0.5654)	Prec@1 85.714 (85.714)	
2022-01-29 12:54:24 - INFO - 
 Epoch: 12	Training Loss 1.4419 	Training Prec@1 62.500 	Validation Loss 0.5654 	Validation Prec@1 85.714 	
2022-01-29 12:54:24 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:24 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:24 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:24 - INFO - TRAINING - Epoch: [12][0/2]	Time 0.244 (0.244)	Data 0.240 (0.240)	Loss 1.0297 (1.0297)	Prec@1 65.625 (65.625)	
2022-01-29 12:54:24 - INFO - EVALUATING - Epoch: [12][0/1]	Time 0.194 (0.194)	Data 0.192 (0.192)	Loss 0.5553 (0.5553)	Prec@1 80.952 (80.952)	
2022-01-29 12:54:24 - INFO - 
 Epoch: 13	Training Loss 1.2294 	Training Prec@1 62.500 	Validation Loss 0.5553 	Validation Prec@1 80.952 	
2022-01-29 12:54:24 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:24 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:24 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:24 - INFO - TRAINING - Epoch: [13][0/2]	Time 0.198 (0.198)	Data 0.195 (0.195)	Loss 1.1544 (1.1544)	Prec@1 54.688 (54.688)	
2022-01-29 12:54:24 - INFO - EVALUATING - Epoch: [13][0/1]	Time 0.199 (0.199)	Data 0.197 (0.197)	Loss 0.6312 (0.6312)	Prec@1 76.190 (76.190)	
2022-01-29 12:54:24 - INFO - 
 Epoch: 14	Training Loss 1.2085 	Training Prec@1 52.500 	Validation Loss 0.6312 	Validation Prec@1 76.190 	
2022-01-29 12:54:25 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:25 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:25 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:25 - INFO - TRAINING - Epoch: [14][0/2]	Time 0.245 (0.245)	Data 0.241 (0.241)	Loss 1.0442 (1.0442)	Prec@1 59.375 (59.375)	
2022-01-29 12:54:25 - INFO - EVALUATING - Epoch: [14][0/1]	Time 0.193 (0.193)	Data 0.192 (0.192)	Loss 0.3405 (0.3405)	Prec@1 90.476 (90.476)	
2022-01-29 12:54:25 - INFO - 
 Epoch: 15	Training Loss 1.1386 	Training Prec@1 60.000 	Validation Loss 0.3405 	Validation Prec@1 90.476 	
2022-01-29 12:54:25 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:25 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:25 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:25 - INFO - TRAINING - Epoch: [15][0/2]	Time 0.241 (0.241)	Data 0.237 (0.237)	Loss 0.6714 (0.6714)	Prec@1 68.750 (68.750)	
2022-01-29 12:54:25 - INFO - EVALUATING - Epoch: [15][0/1]	Time 0.193 (0.193)	Data 0.191 (0.191)	Loss 0.7089 (0.7089)	Prec@1 85.714 (85.714)	
2022-01-29 12:54:25 - INFO - 
 Epoch: 16	Training Loss 0.7099 	Training Prec@1 67.500 	Validation Loss 0.7089 	Validation Prec@1 85.714 	
2022-01-29 12:54:25 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:25 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:25 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:26 - INFO - TRAINING - Epoch: [16][0/2]	Time 0.203 (0.203)	Data 0.199 (0.199)	Loss 1.3119 (1.3119)	Prec@1 68.750 (68.750)	
2022-01-29 12:54:26 - INFO - EVALUATING - Epoch: [16][0/1]	Time 0.200 (0.200)	Data 0.198 (0.198)	Loss 0.3368 (0.3368)	Prec@1 95.238 (95.238)	
2022-01-29 12:54:26 - INFO - 
 Epoch: 17	Training Loss 1.1589 	Training Prec@1 72.500 	Validation Loss 0.3368 	Validation Prec@1 95.238 	
2022-01-29 12:54:26 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:26 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:26 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:26 - INFO - TRAINING - Epoch: [17][0/2]	Time 0.192 (0.192)	Data 0.189 (0.189)	Loss 0.9079 (0.9079)	Prec@1 73.438 (73.438)	
2022-01-29 12:54:26 - INFO - EVALUATING - Epoch: [17][0/1]	Time 0.192 (0.192)	Data 0.191 (0.191)	Loss 1.0000 (1.0000)	Prec@1 76.190 (76.190)	
2022-01-29 12:54:26 - INFO - 
 Epoch: 18	Training Loss 0.8916 	Training Prec@1 73.750 	Validation Loss 1.0000 	Validation Prec@1 76.190 	
2022-01-29 12:54:26 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:26 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:26 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:27 - INFO - TRAINING - Epoch: [18][0/2]	Time 0.196 (0.196)	Data 0.193 (0.193)	Loss 1.2365 (1.2365)	Prec@1 70.312 (70.312)	
2022-01-29 12:54:27 - INFO - EVALUATING - Epoch: [18][0/1]	Time 0.198 (0.198)	Data 0.196 (0.196)	Loss 0.7892 (0.7892)	Prec@1 85.714 (85.714)	
2022-01-29 12:54:27 - INFO - 
 Epoch: 19	Training Loss 1.1469 	Training Prec@1 71.250 	Validation Loss 0.7892 	Validation Prec@1 85.714 	
2022-01-29 12:54:27 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:27 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:27 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:27 - INFO - TRAINING - Epoch: [19][0/2]	Time 0.196 (0.196)	Data 0.192 (0.192)	Loss 1.5353 (1.5353)	Prec@1 59.375 (59.375)	
2022-01-29 12:54:27 - INFO - EVALUATING - Epoch: [19][0/1]	Time 0.189 (0.189)	Data 0.187 (0.187)	Loss 0.3351 (0.3351)	Prec@1 90.476 (90.476)	
2022-01-29 12:54:27 - INFO - 
 Epoch: 20	Training Loss 1.4387 	Training Prec@1 60.000 	Validation Loss 0.3351 	Validation Prec@1 90.476 	
2022-01-29 12:54:27 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:27 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:27 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:27 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:27 - INFO - TRAINING - Epoch: [20][0/2]	Time 0.195 (0.195)	Data 0.192 (0.192)	Loss 1.0026 (1.0026)	Prec@1 54.688 (54.688)	
2022-01-29 12:54:28 - INFO - EVALUATING - Epoch: [20][0/1]	Time 0.188 (0.188)	Data 0.186 (0.186)	Loss 0.4089 (0.4089)	Prec@1 85.714 (85.714)	
2022-01-29 12:54:28 - INFO - 
 Epoch: 21	Training Loss 0.9999 	Training Prec@1 58.750 	Validation Loss 0.4089 	Validation Prec@1 85.714 	
2022-01-29 12:54:28 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:28 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:28 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:28 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:28 - INFO - TRAINING - Epoch: [21][0/2]	Time 0.192 (0.192)	Data 0.189 (0.189)	Loss 1.0836 (1.0836)	Prec@1 53.125 (53.125)	
2022-01-29 12:54:28 - INFO - EVALUATING - Epoch: [21][0/1]	Time 0.234 (0.234)	Data 0.232 (0.232)	Loss 0.3352 (0.3352)	Prec@1 90.476 (90.476)	
2022-01-29 12:54:28 - INFO - 
 Epoch: 22	Training Loss 1.0230 	Training Prec@1 57.500 	Validation Loss 0.3352 	Validation Prec@1 90.476 	
2022-01-29 12:54:28 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:28 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:28 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:28 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:28 - INFO - TRAINING - Epoch: [22][0/2]	Time 0.199 (0.199)	Data 0.195 (0.195)	Loss 0.6993 (0.6993)	Prec@1 76.562 (76.562)	
2022-01-29 12:54:28 - INFO - EVALUATING - Epoch: [22][0/1]	Time 0.197 (0.197)	Data 0.195 (0.195)	Loss 0.3116 (0.3116)	Prec@1 90.476 (90.476)	
2022-01-29 12:54:28 - INFO - 
 Epoch: 23	Training Loss 0.6628 	Training Prec@1 78.750 	Validation Loss 0.3116 	Validation Prec@1 90.476 	
2022-01-29 12:54:28 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:28 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:28 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:28 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:29 - INFO - TRAINING - Epoch: [23][0/2]	Time 0.239 (0.239)	Data 0.235 (0.235)	Loss 0.8679 (0.8679)	Prec@1 60.938 (60.938)	
2022-01-29 12:54:29 - INFO - EVALUATING - Epoch: [23][0/1]	Time 0.193 (0.193)	Data 0.191 (0.191)	Loss 0.3231 (0.3231)	Prec@1 90.476 (90.476)	
2022-01-29 12:54:29 - INFO - 
 Epoch: 24	Training Loss 0.8856 	Training Prec@1 62.500 	Validation Loss 0.3231 	Validation Prec@1 90.476 	
2022-01-29 12:54:29 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:29 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:29 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:29 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:29 - INFO - TRAINING - Epoch: [24][0/2]	Time 0.193 (0.193)	Data 0.190 (0.190)	Loss 0.6308 (0.6308)	Prec@1 81.250 (81.250)	
2022-01-29 12:54:29 - INFO - EVALUATING - Epoch: [24][0/1]	Time 0.198 (0.198)	Data 0.196 (0.196)	Loss 0.3577 (0.3577)	Prec@1 90.476 (90.476)	
2022-01-29 12:54:29 - INFO - 
 Epoch: 25	Training Loss 0.6222 	Training Prec@1 81.250 	Validation Loss 0.3577 	Validation Prec@1 90.476 	
2022-01-29 12:54:29 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:29 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:29 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:29 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:30 - INFO - TRAINING - Epoch: [25][0/2]	Time 0.198 (0.198)	Data 0.194 (0.194)	Loss 0.7071 (0.7071)	Prec@1 71.875 (71.875)	
2022-01-29 12:54:30 - INFO - EVALUATING - Epoch: [25][0/1]	Time 0.198 (0.198)	Data 0.196 (0.196)	Loss 0.3385 (0.3385)	Prec@1 90.476 (90.476)	
2022-01-29 12:54:30 - INFO - 
 Epoch: 26	Training Loss 0.6964 	Training Prec@1 73.750 	Validation Loss 0.3385 	Validation Prec@1 90.476 	
2022-01-29 12:54:30 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:30 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:30 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:30 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:30 - INFO - TRAINING - Epoch: [26][0/2]	Time 0.197 (0.197)	Data 0.193 (0.193)	Loss 0.7472 (0.7472)	Prec@1 75.000 (75.000)	
2022-01-29 12:54:30 - INFO - EVALUATING - Epoch: [26][0/1]	Time 0.201 (0.201)	Data 0.200 (0.200)	Loss 0.3676 (0.3676)	Prec@1 85.714 (85.714)	
2022-01-29 12:54:30 - INFO - 
 Epoch: 27	Training Loss 0.8165 	Training Prec@1 75.000 	Validation Loss 0.3676 	Validation Prec@1 85.714 	
2022-01-29 12:54:30 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:30 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:30 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:30 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:30 - INFO - TRAINING - Epoch: [27][0/2]	Time 0.197 (0.197)	Data 0.193 (0.193)	Loss 0.7256 (0.7256)	Prec@1 68.750 (68.750)	
2022-01-29 12:54:31 - INFO - EVALUATING - Epoch: [27][0/1]	Time 0.198 (0.198)	Data 0.196 (0.196)	Loss 0.2401 (0.2401)	Prec@1 100.000 (100.000)	
2022-01-29 12:54:31 - INFO - 
 Epoch: 28	Training Loss 0.7361 	Training Prec@1 70.000 	Validation Loss 0.2401 	Validation Prec@1 100.000 	
2022-01-29 12:54:31 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:31 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:31 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:31 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:31 - INFO - TRAINING - Epoch: [28][0/2]	Time 0.244 (0.244)	Data 0.240 (0.240)	Loss 0.5787 (0.5787)	Prec@1 81.250 (81.250)	
2022-01-29 12:54:31 - INFO - EVALUATING - Epoch: [28][0/1]	Time 0.197 (0.197)	Data 0.195 (0.195)	Loss 0.4929 (0.4929)	Prec@1 90.476 (90.476)	
2022-01-29 12:54:31 - INFO - 
 Epoch: 29	Training Loss 0.6358 	Training Prec@1 80.000 	Validation Loss 0.4929 	Validation Prec@1 90.476 	
2022-01-29 12:54:31 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:31 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:31 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:31 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:31 - INFO - TRAINING - Epoch: [29][0/2]	Time 0.191 (0.191)	Data 0.188 (0.188)	Loss 0.6660 (0.6660)	Prec@1 79.688 (79.688)	
2022-01-29 12:54:32 - INFO - EVALUATING - Epoch: [29][0/1]	Time 0.198 (0.198)	Data 0.196 (0.196)	Loss 0.3921 (0.3921)	Prec@1 90.476 (90.476)	
2022-01-29 12:54:32 - INFO - 
 Epoch: 30	Training Loss 0.7469 	Training Prec@1 76.250 	Validation Loss 0.3921 	Validation Prec@1 90.476 	
2022-01-29 12:54:32 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:32 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:32 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:32 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:32 - INFO - TRAINING - Epoch: [30][0/2]	Time 0.222 (0.222)	Data 0.219 (0.219)	Loss 0.5475 (0.5475)	Prec@1 81.250 (81.250)	
2022-01-29 12:54:32 - INFO - EVALUATING - Epoch: [30][0/1]	Time 0.198 (0.198)	Data 0.196 (0.196)	Loss 0.4687 (0.4687)	Prec@1 85.714 (85.714)	
2022-01-29 12:54:32 - INFO - 
 Epoch: 31	Training Loss 0.6794 	Training Prec@1 77.500 	Validation Loss 0.4687 	Validation Prec@1 85.714 	
2022-01-29 12:54:32 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:32 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:32 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:32 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:32 - INFO - TRAINING - Epoch: [31][0/2]	Time 0.238 (0.238)	Data 0.234 (0.234)	Loss 0.6010 (0.6010)	Prec@1 81.250 (81.250)	
2022-01-29 12:54:33 - INFO - EVALUATING - Epoch: [31][0/1]	Time 0.234 (0.234)	Data 0.232 (0.232)	Loss 0.3764 (0.3764)	Prec@1 90.476 (90.476)	
2022-01-29 12:54:33 - INFO - 
 Epoch: 32	Training Loss 0.6463 	Training Prec@1 78.750 	Validation Loss 0.3764 	Validation Prec@1 90.476 	
2022-01-29 12:54:33 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:33 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:33 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:33 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:33 - INFO - TRAINING - Epoch: [32][0/2]	Time 0.202 (0.202)	Data 0.198 (0.198)	Loss 0.5739 (0.5739)	Prec@1 82.812 (82.812)	
2022-01-29 12:54:33 - INFO - EVALUATING - Epoch: [32][0/1]	Time 0.134 (0.134)	Data 0.132 (0.132)	Loss 0.2602 (0.2602)	Prec@1 100.000 (100.000)	
2022-01-29 12:54:33 - INFO - 
 Epoch: 33	Training Loss 0.6805 	Training Prec@1 78.750 	Validation Loss 0.2602 	Validation Prec@1 100.000 	
2022-01-29 12:54:33 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:33 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:33 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:33 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:33 - INFO - TRAINING - Epoch: [33][0/2]	Time 0.201 (0.201)	Data 0.197 (0.197)	Loss 0.5900 (0.5900)	Prec@1 81.250 (81.250)	
2022-01-29 12:54:33 - INFO - EVALUATING - Epoch: [33][0/1]	Time 0.197 (0.197)	Data 0.195 (0.195)	Loss 0.2013 (0.2013)	Prec@1 100.000 (100.000)	
2022-01-29 12:54:33 - INFO - 
 Epoch: 34	Training Loss 0.5601 	Training Prec@1 82.500 	Validation Loss 0.2013 	Validation Prec@1 100.000 	
2022-01-29 12:54:33 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:33 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:33 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:33 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:34 - INFO - TRAINING - Epoch: [34][0/2]	Time 0.198 (0.198)	Data 0.194 (0.194)	Loss 0.5651 (0.5651)	Prec@1 84.375 (84.375)	
2022-01-29 12:54:34 - INFO - EVALUATING - Epoch: [34][0/1]	Time 0.197 (0.197)	Data 0.195 (0.195)	Loss 0.2612 (0.2612)	Prec@1 100.000 (100.000)	
2022-01-29 12:54:34 - INFO - 
 Epoch: 35	Training Loss 0.6236 	Training Prec@1 81.250 	Validation Loss 0.2612 	Validation Prec@1 100.000 	
2022-01-29 12:54:34 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:34 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:34 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:34 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:34 - INFO - TRAINING - Epoch: [35][0/2]	Time 0.192 (0.192)	Data 0.188 (0.188)	Loss 0.5077 (0.5077)	Prec@1 87.500 (87.500)	
2022-01-29 12:54:34 - INFO - EVALUATING - Epoch: [35][0/1]	Time 0.128 (0.128)	Data 0.127 (0.127)	Loss 0.1241 (0.1241)	Prec@1 100.000 (100.000)	
2022-01-29 12:54:34 - INFO - 
 Epoch: 36	Training Loss 0.6088 	Training Prec@1 83.750 	Validation Loss 0.1241 	Validation Prec@1 100.000 	
2022-01-29 12:54:34 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:34 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:34 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:34 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:34 - INFO - TRAINING - Epoch: [36][0/2]	Time 0.196 (0.196)	Data 0.192 (0.192)	Loss 0.5042 (0.5042)	Prec@1 85.938 (85.938)	
2022-01-29 12:54:35 - INFO - EVALUATING - Epoch: [36][0/1]	Time 0.238 (0.238)	Data 0.236 (0.236)	Loss 0.1385 (0.1385)	Prec@1 100.000 (100.000)	
2022-01-29 12:54:35 - INFO - 
 Epoch: 37	Training Loss 0.6697 	Training Prec@1 82.500 	Validation Loss 0.1385 	Validation Prec@1 100.000 	
2022-01-29 12:54:35 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:35 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:35 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:35 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:35 - INFO - TRAINING - Epoch: [37][0/2]	Time 0.192 (0.192)	Data 0.188 (0.188)	Loss 0.5603 (0.5603)	Prec@1 79.688 (79.688)	
2022-01-29 12:54:35 - INFO - EVALUATING - Epoch: [37][0/1]	Time 0.201 (0.201)	Data 0.199 (0.199)	Loss 0.3425 (0.3425)	Prec@1 95.238 (95.238)	
2022-01-29 12:54:35 - INFO - 
 Epoch: 38	Training Loss 0.7322 	Training Prec@1 77.500 	Validation Loss 0.3425 	Validation Prec@1 95.238 	
2022-01-29 12:54:35 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:35 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:35 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:35 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:35 - INFO - TRAINING - Epoch: [38][0/2]	Time 0.238 (0.238)	Data 0.235 (0.235)	Loss 0.5961 (0.5961)	Prec@1 81.250 (81.250)	
2022-01-29 12:54:36 - INFO - EVALUATING - Epoch: [38][0/1]	Time 0.197 (0.197)	Data 0.195 (0.195)	Loss 0.3130 (0.3130)	Prec@1 90.476 (90.476)	
2022-01-29 12:54:36 - INFO - 
 Epoch: 39	Training Loss 0.5672 	Training Prec@1 82.500 	Validation Loss 0.3130 	Validation Prec@1 90.476 	
2022-01-29 12:54:36 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:36 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:36 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:36 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:36 - INFO - TRAINING - Epoch: [39][0/2]	Time 0.202 (0.202)	Data 0.198 (0.198)	Loss 0.4804 (0.4804)	Prec@1 84.375 (84.375)	
2022-01-29 12:54:36 - INFO - EVALUATING - Epoch: [39][0/1]	Time 0.198 (0.198)	Data 0.196 (0.196)	Loss 0.4739 (0.4739)	Prec@1 80.952 (80.952)	
2022-01-29 12:54:36 - INFO - 
 Epoch: 40	Training Loss 0.6102 	Training Prec@1 83.750 	Validation Loss 0.4739 	Validation Prec@1 80.952 	
2022-01-29 12:54:36 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:36 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:36 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:36 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:36 - INFO - TRAINING - Epoch: [40][0/2]	Time 0.197 (0.197)	Data 0.193 (0.193)	Loss 0.7625 (0.7625)	Prec@1 75.000 (75.000)	
2022-01-29 12:54:37 - INFO - EVALUATING - Epoch: [40][0/1]	Time 0.197 (0.197)	Data 0.196 (0.196)	Loss 0.3279 (0.3279)	Prec@1 95.238 (95.238)	
2022-01-29 12:54:37 - INFO - 
 Epoch: 41	Training Loss 0.6831 	Training Prec@1 77.500 	Validation Loss 0.3279 	Validation Prec@1 95.238 	
2022-01-29 12:54:37 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:37 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:37 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:37 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:37 - INFO - TRAINING - Epoch: [41][0/2]	Time 0.204 (0.204)	Data 0.201 (0.201)	Loss 0.6346 (0.6346)	Prec@1 78.125 (78.125)	
2022-01-29 12:54:37 - INFO - EVALUATING - Epoch: [41][0/1]	Time 0.197 (0.197)	Data 0.196 (0.196)	Loss 0.6854 (0.6854)	Prec@1 80.952 (80.952)	
2022-01-29 12:54:37 - INFO - 
 Epoch: 42	Training Loss 0.6548 	Training Prec@1 77.500 	Validation Loss 0.6854 	Validation Prec@1 80.952 	
2022-01-29 12:54:37 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:37 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:37 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:37 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:37 - INFO - TRAINING - Epoch: [42][0/2]	Time 0.197 (0.197)	Data 0.193 (0.193)	Loss 0.8582 (0.8582)	Prec@1 73.438 (73.438)	
2022-01-29 12:54:37 - INFO - EVALUATING - Epoch: [42][0/1]	Time 0.197 (0.197)	Data 0.195 (0.195)	Loss 0.1612 (0.1612)	Prec@1 100.000 (100.000)	
2022-01-29 12:54:37 - INFO - 
 Epoch: 43	Training Loss 0.7761 	Training Prec@1 76.250 	Validation Loss 0.1612 	Validation Prec@1 100.000 	
2022-01-29 12:54:37 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:37 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:37 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:37 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:38 - INFO - TRAINING - Epoch: [43][0/2]	Time 0.193 (0.193)	Data 0.190 (0.190)	Loss 0.6452 (0.6452)	Prec@1 81.250 (81.250)	
2022-01-29 12:54:38 - INFO - EVALUATING - Epoch: [43][0/1]	Time 0.198 (0.198)	Data 0.196 (0.196)	Loss 0.2667 (0.2667)	Prec@1 95.238 (95.238)	
2022-01-29 12:54:38 - INFO - 
 Epoch: 44	Training Loss 0.6330 	Training Prec@1 80.000 	Validation Loss 0.2667 	Validation Prec@1 95.238 	
2022-01-29 12:54:38 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:38 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:38 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:38 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:38 - INFO - TRAINING - Epoch: [44][0/2]	Time 0.235 (0.235)	Data 0.232 (0.232)	Loss 0.4548 (0.4548)	Prec@1 79.688 (79.688)	
2022-01-29 12:54:38 - INFO - EVALUATING - Epoch: [44][0/1]	Time 0.197 (0.197)	Data 0.196 (0.196)	Loss 0.2669 (0.2669)	Prec@1 95.238 (95.238)	
2022-01-29 12:54:38 - INFO - 
 Epoch: 45	Training Loss 0.5020 	Training Prec@1 80.000 	Validation Loss 0.2669 	Validation Prec@1 95.238 	
2022-01-29 12:54:38 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:38 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:38 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:38 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:39 - INFO - TRAINING - Epoch: [45][0/2]	Time 0.239 (0.239)	Data 0.235 (0.235)	Loss 0.5036 (0.5036)	Prec@1 76.562 (76.562)	
2022-01-29 12:54:39 - INFO - EVALUATING - Epoch: [45][0/1]	Time 0.197 (0.197)	Data 0.195 (0.195)	Loss 0.2820 (0.2820)	Prec@1 95.238 (95.238)	
2022-01-29 12:54:39 - INFO - 
 Epoch: 46	Training Loss 0.5096 	Training Prec@1 77.500 	Validation Loss 0.2820 	Validation Prec@1 95.238 	
2022-01-29 12:54:39 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:39 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:39 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:39 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:39 - INFO - TRAINING - Epoch: [46][0/2]	Time 0.203 (0.203)	Data 0.199 (0.199)	Loss 0.5329 (0.5329)	Prec@1 79.688 (79.688)	
2022-01-29 12:54:39 - INFO - EVALUATING - Epoch: [46][0/1]	Time 0.197 (0.197)	Data 0.195 (0.195)	Loss 0.2242 (0.2242)	Prec@1 95.238 (95.238)	
2022-01-29 12:54:39 - INFO - 
 Epoch: 47	Training Loss 0.5641 	Training Prec@1 77.500 	Validation Loss 0.2242 	Validation Prec@1 95.238 	
2022-01-29 12:54:39 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:39 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:39 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:39 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:40 - INFO - TRAINING - Epoch: [47][0/2]	Time 0.202 (0.202)	Data 0.198 (0.198)	Loss 0.5472 (0.5472)	Prec@1 75.000 (75.000)	
2022-01-29 12:54:40 - INFO - EVALUATING - Epoch: [47][0/1]	Time 0.198 (0.198)	Data 0.196 (0.196)	Loss 0.1836 (0.1836)	Prec@1 95.238 (95.238)	
2022-01-29 12:54:40 - INFO - 
 Epoch: 48	Training Loss 0.5567 	Training Prec@1 76.250 	Validation Loss 0.1836 	Validation Prec@1 95.238 	
2022-01-29 12:54:40 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:40 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:40 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:40 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:40 - INFO - TRAINING - Epoch: [48][0/2]	Time 0.193 (0.193)	Data 0.190 (0.190)	Loss 0.5274 (0.5274)	Prec@1 84.375 (84.375)	
2022-01-29 12:54:40 - INFO - EVALUATING - Epoch: [48][0/1]	Time 0.197 (0.197)	Data 0.195 (0.195)	Loss 0.2270 (0.2270)	Prec@1 95.238 (95.238)	
2022-01-29 12:54:40 - INFO - 
 Epoch: 49	Training Loss 0.6440 	Training Prec@1 80.000 	Validation Loss 0.2270 	Validation Prec@1 95.238 	
2022-01-29 12:54:40 - DEBUG - OPTIMIZER - setting method = Adam
2022-01-29 12:54:40 - DEBUG - OPTIMIZER - setting lr = 0.025
2022-01-29 12:54:40 - DEBUG - OPTIMIZER - setting betas = (0.9, 0.999)
2022-01-29 12:54:40 - DEBUG - OPTIMIZER - setting lr = 0.0025
2022-01-29 12:54:40 - INFO - TRAINING - Epoch: [49][0/2]	Time 0.202 (0.202)	Data 0.198 (0.198)	Loss 0.4857 (0.4857)	Prec@1 81.250 (81.250)	
2022-01-29 12:54:41 - INFO - EVALUATING - Epoch: [49][0/1]	Time 0.197 (0.197)	Data 0.196 (0.196)	Loss 0.1965 (0.1965)	Prec@1 95.238 (95.238)	
2022-01-29 12:54:41 - INFO - 
 Epoch: 50	Training Loss 0.5250 	Training Prec@1 81.250 	Validation Loss 0.1965 	Validation Prec@1 95.238 	
